#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Test compound words vÃ  multi-word expressions trong tiáº¿ng Viá»‡t
"""

from EnhancedSearchEngine_Fixed import FixedEnhancedSearchEngine

def test_compound_words():
    """Test cÃ¡c tá»« ghÃ©p tiáº¿ng Viá»‡t"""
    
    print("ğŸ‡»ğŸ‡³ TESTING VIETNAMESE COMPOUND WORDS")
    print("=" * 60)
    
    engine = FixedEnhancedSearchEngine('data_content.json')
    engine.build_index()
    
    # Test cases cho tá»« ghÃ©p tiáº¿ng Viá»‡t
    test_cases = [
        {
            'query': 'Viá»‡t Nam',
            'description': 'Tá»« ghÃ©p cÃ³ nghÄ©a Ä‘áº·c biá»‡t (country name)',
            'expected': 'Should find Vietnam-related documents'
        },
        {
            'query': 'viá»‡t nam',  # lowercase
            'description': 'Tá»« ghÃ©p viáº¿t thÆ°á»ng',
            'expected': 'Should match regardless of case'
        },
        {
            'query': 'Viá»‡t',
            'description': 'Chá»‰ ná»­a tá»« ghÃ©p (fragment)',
            'expected': 'May find documents with Viá»‡t Nam or other Viá»‡t* words'
        },
        {
            'query': 'Nam',
            'description': 'Chá»‰ ná»­a tá»« ghÃ©p (fragment)',
            'expected': 'May find many documents (common word)'
        },
        {
            'query': 'Há»“ ChÃ­ Minh',
            'description': 'TÃªn ngÆ°á»i (3 tá»«)',
            'expected': 'Should find Ho Chi Minh documents'
        },
        {
            'query': 'Há»“ ChÃ­',
            'description': 'TÃªn khÃ´ng Ä‘áº§y Ä‘á»§',
            'expected': 'Should still find relevant documents'
        },
        {
            'query': 'khá»Ÿi nghÄ©a',
            'description': 'Tá»« ghÃ©p Ä‘á»™ng tá»«',
            'expected': 'Should find uprising/rebellion documents'
        },
        {
            'query': 'khá»Ÿi',
            'description': 'Chá»‰ má»™t pháº§n cá»§a tá»« ghÃ©p',
            'expected': 'May miss some contexts where full phrase is important'
        },
        {
            'query': 'Äiá»‡n BiÃªn Phá»§',
            'description': 'TÃªn Ä‘á»‹a danh (3 tá»«)',
            'expected': 'Should find battle/location documents'
        },
        {
            'query': 'Äiá»‡n BiÃªn',
            'description': 'TÃªn Ä‘á»‹a danh khÃ´ng Ä‘áº§y Ä‘á»§',
            'expected': 'Should still find relevant documents'
        }
    ]
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n[{i}] Testing: '{test_case['query']}'")
        print(f"    Description: {test_case['description']}")
        print(f"    Expected: {test_case['expected']}")
        
        # Search with compound word
        results = engine.search(test_case['query'], top_k=3)
        
        print(f"    Results: {len(results)} found")
        for j, result in enumerate(results[:3], 1):
            score = result['score']
            file_name = result['file_name']
            preview = result['preview'][:100]
            
            print(f"      [{j}] {file_name} (Score: {score:.4f})")
            print(f"          Preview: {preview}...")
            
            # Analyze token matching
            query_tokens = engine.tokenizer.tokenize(test_case['query'])
            preview_tokens = engine.tokenizer.tokenize(preview)
            matches = set(query_tokens) & set(preview_tokens)
            
            print(f"          Query tokens: {query_tokens}")
            print(f"          Matching: {list(matches)} ({len(matches)}/{len(query_tokens)})")
        
        print("-" * 50)

def analyze_compound_word_issues():
    """PhÃ¢n tÃ­ch cÃ¡c váº¥n Ä‘á» vá»›i tá»« ghÃ©p"""
    
    print("\nğŸ” COMPOUND WORD ANALYSIS")
    print("=" * 60)
    
    engine = FixedEnhancedSearchEngine('data_content.json')
    engine.build_index()
    
    # So sÃ¡nh tá»« ghÃ©p vs tá»« riÃªng láº»
    comparisons = [
        ('Viá»‡t Nam', ['Viá»‡t', 'Nam']),
        ('Há»“ ChÃ­ Minh', ['Há»“', 'ChÃ­', 'Minh']),
        ('khá»Ÿi nghÄ©a', ['khá»Ÿi', 'nghÄ©a']),
        ('Äiá»‡n BiÃªn Phá»§', ['Äiá»‡n', 'BiÃªn', 'Phá»§']),
        ('chiáº¿n tranh', ['chiáº¿n', 'tranh']),
        ('cÃ¡ch máº¡ng', ['cÃ¡ch', 'máº¡ng'])
    ]
    
    for compound, components in comparisons:
        print(f"\nğŸ“ Analyzing: '{compound}' vs {components}")
        
        # Test compound word
        compound_results = engine.search(compound, top_k=3)
        compound_scores = [r['score'] for r in compound_results]
        
        print(f"   Compound query '{compound}':")
        print(f"   â†’ Scores: {[f'{s:.4f}' for s in compound_scores[:3]]}")
        
        # Test individual components
        for component in components:
            component_results = engine.search(component, top_k=3)
            component_scores = [r['score'] for r in component_results]
            print(f"   Component '{component}':")
            print(f"   â†’ Scores: {[f'{s:.4f}' for s in component_scores[:3]]}")
        
        # Test combined search (all components)
        combined_query = ' '.join(components)
        combined_results = engine.search(combined_query, top_k=3) 
        combined_scores = [r['score'] for r in combined_results]
        print(f"   Combined '{combined_query}':")
        print(f"   â†’ Scores: {[f'{s:.4f}' for s in combined_scores[:3]]}")
        
        print("-" * 40)

def test_ngram_approach():
    """Test cÃ¡ch tiáº¿p cáº­n n-gram cho tá»« ghÃ©p"""
    
    print("\nğŸ”¬ N-GRAM APPROACH TEST")
    print("=" * 60)
    
    # Sample text processing
    sample_texts = [
        "Viá»‡t Nam lÃ  má»™t quá»‘c gia á»Ÿ ÄÃ´ng Nam Ã",
        "Há»“ ChÃ­ Minh lÃ  lÃ£nh tá»¥ cá»§a cÃ¡ch máº¡ng Viá»‡t Nam", 
        "Chiáº¿n dá»‹ch Äiá»‡n BiÃªn Phá»§ diá»…n ra nÄƒm 1954",
        "Khá»Ÿi nghÄ©a BÃ  Triá»‡u chá»‘ng láº¡i quÃ¢n Nam HÃ¡n"
    ]
    
    def extract_bigrams(text, tokenizer):
        """Extract bigrams from text"""
        tokens = tokenizer.tokenize(text.lower())
        bigrams = []
        for i in range(len(tokens) - 1):
            bigrams.append(f"{tokens[i]} {tokens[i+1]}")
        return bigrams
    
    def extract_trigrams(text, tokenizer):
        """Extract trigrams from text"""
        tokens = tokenizer.tokenize(text.lower())
        trigrams = []
        for i in range(len(tokens) - 2):
            trigrams.append(f"{tokens[i]} {tokens[i+1]} {tokens[i+2]}")
        return trigrams
    
    engine = FixedEnhancedSearchEngine('data_content.json')
    
    for text in sample_texts:
        print(f"\nText: '{text}'")
        
        # Regular tokens
        tokens = engine.tokenizer.tokenize(text.lower())
        print(f"Tokens: {tokens}")
        
        # Bigrams
        bigrams = extract_bigrams(text, engine.tokenizer)
        print(f"Bigrams: {bigrams}")
        
        # Trigrams  
        trigrams = extract_trigrams(text, engine.tokenizer)
        print(f"Trigrams: {trigrams}")

if __name__ == "__main__":
    test_compound_words()
    analyze_compound_word_issues()
    test_ngram_approach()