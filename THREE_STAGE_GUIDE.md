# ğŸ”¬ Three-Stage Retrieval Engine - HÆ°á»›ng Dáº«n Chi Tiáº¿t

## ğŸ“š **Paper Gá»‘c**
**"Text Retrieval with Multi-Stage Re-Ranking Models"**  
*Yuichi Sasazawa, Kenichi Yokote, Osamu Imaichi, Yasuhiro Sogawa*  
*Hitachi, Ltd. Research and Development Group*  
*arXiv:2311.07994v1 [cs.IR] 14 Nov 2023*

## ğŸ¯ Tá»•ng Quan

**Three-Stage Retrieval** lÃ  má»™t kiáº¿n trÃºc tÃ¬m kiáº¿m tiÃªn tiáº¿n Ä‘Æ°á»£c implement theo paper há»c thuáº­t tá»« Hitachi, chia quÃ¡ trÃ¬nh retrieval thÃ nh 3 giai Ä‘oáº¡n tuáº§n tá»± Ä‘á»ƒ tá»‘i Æ°u hÃ³a cáº£ tá»‘c Ä‘á»™ vÃ  Ä‘á»™ chÃ­nh xÃ¡c.

### ğŸ“Š **Quy TrÃ¬nh 3 Giai Äoáº¡n (Theo Paper Hitachi):**

```
ğŸ“„ Corpus (aâ‚€ documents - toÃ n bá»™ corpus)
         â†“
ğŸ” Stage 1: BM25 Retrieval â†’ aâ‚ candidates (thÆ°á»ng 100-1000)
         â†“
ğŸ§  Stage 2: Language Model â†’ aâ‚‚ candidates (thÆ°á»ng 10-50)  
         â†“
ğŸ¯ Stage 3: High-Performance Model â†’ final results (top 10-20)
```

**Má»¥c TiÃªu Paper:** Cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c tÃ¬m kiáº¿m while minimizing search delay báº±ng cÃ¡ch chá»‰ Ã¡p dá»¥ng expensive models cho limited number of highly relevant documents.

## ğŸ—ï¸ Kiáº¿n TrÃºc Chi Tiáº¿t (Theo Paper Hitachi)

### **ğŸ” Stage 1: BM25 Retrieval**
```python
Purpose: Fast vocabulary-based search (low accuracy, high speed)
Input: aâ‚€ documents (toÃ n bá»™ corpus)
Algorithm: BM25 (Robertson et al., 1995) 
Output: aâ‚ documents (thÆ°á»ng 100-1000)
Speed: ~0.0178s/query (fastest)
Strength: Fast filtering, good cho exact keyword matches
```

**Paper Implementation:**
- Sá»­ dá»¥ng Pyserini toolkit Ä‘á»ƒ tÃ­nh BM25 similarity
- BM25 calculates similarity cho ALL documents trong corpus
- Ranks documents theo similarity order
- Output: top aâ‚ documents with highest similarity

### **ğŸ§  Stage 2: Language Model (Cross-Encoder)**  
```python
Purpose: Medium accuracy, medium speed re-ranking
Input: aâ‚ candidates tá»« Stage 1 (100-1000 docs)
Algorithm: Cross-encoder Language Model (MiniLM)
Model: MiniLM (30M parameters, 6 layers, 384 hidden size)
Output: aâ‚‚ documents (thÆ°á»ng 10-50)
Speed: ~0.1939s/query (moderate)
Strength: Better understanding cá»§a query-document relationships
```

**Paper Implementation:**
- Cross-encoder nháº­n concatenation cá»§a query vÃ  document
- Model outputs hidden state of [CLS] token â†’ classification layer
- Training: binary classification vá»›i cross-entropy loss
- Inference: scalar value tá»« classification layer = similarity score
- Re-ranks chá»‰ top aâ‚ documents (khÃ´ng pháº£i toÃ n bá»™ corpus)

### **ğŸ¯ Stage 3: High-Performance Model**
```python
Purpose: Highest accuracy, lowest speed (chá»‰ cho top candidates)
Input: aâ‚‚ candidates tá»« Stage 2 (10-50 docs)
Algorithm: Model Ensemble HOáº¶C Larger Model HOáº¶C Pairwise Model
Output: Final ranked results (top 10-20)
Speed: Varies by method (1.39x - 4.00x slower than Stage 2)
Strength: Maximum precision cho final ranking
```

**Paper Methods:**

#### **ğŸ¯ Model Ensemble (Paper's Best Method)**
```python
- Multiple MiniLM models (same architecture, different random seeds)
- Average similarity scores across ensemble members
- 3 models in ensemble (paper experiment)
- Performance: NDCG@10 = 0.5922 (out-of-domain average)
- Speed: 1.50x slower than BM25+LM
```

#### **ğŸ¯ Larger Model (Paper's Highest Accuracy)**
```python
- Larger MiniLM: 81M parameters, 6 layers, 768 hidden size
- Same architecture as Stage 2 nhÆ°ng more parameters
- Performance: NDCG@10 = 0.6176 (out-of-domain average)
- Speed: 1.39x slower than BM25+LM
```

#### **ğŸ¯ Pairwise Model (Existing Method - Paper Comparison)**
```python
- Takes query + 2 documents, predicts which is more relevant
- Requires aâ‚‚Ã—(aâ‚‚-1) inferences (quadratic complexity!)
- Example: top 10 docs require 10Ã—9 = 90 inferences
- Problems: Very slow, poor zero-shot performance, token length limits
- Performance: Poor on out-of-domain (0.5105 average)
- Speed: 4.00x slower than BM25+LM
```

## ğŸš€ CÃ¡ch Sá»­ Dá»¥ng

### **1ï¸âƒ£ Basic Usage**
```python
from ThreeStageRetrieval import ThreeStageRetrieval

# Initialize vá»›i config balanced
retriever = ThreeStageRetrieval(
    use_bm25=True,
    use_dense_retrieval=True, 
    use_reranking=True,
    stage1_top_k=100,
    stage2_top_k=50,
    stage3_top_k=20
)

# Index chunks
retriever.index_chunks(chunks, tokenized_chunks, chunk_to_doc_map)

# Search vá»›i detailed results
results = retriever.retrieve_three_stage(
    query="BÃ  Triá»‡u sinh nÄƒm nao",
    query_tokens=['bÃ ', 'triá»‡u', 'sinh', 'nÄƒm', 'nao'],
    final_top_k=10,
    return_stage_details=True
)

# In káº¿t quáº£
print("ğŸ” Pipeline Summary:")
print(f"Initial: {results['pipeline_summary']['initial_candidates']} chunks")
print(f"Stage 1: {results['pipeline_summary']['after_stage1']} candidates")  
print(f"Stage 2: {results['pipeline_summary']['after_stage2']} candidates")
print(f"Final: {results['pipeline_summary']['final_results']} results")
```

### **2ï¸âƒ£ Paper-based Configurations**
```python
from ThreeStageRetrieval import create_three_stage_config

# Paper's best method: BM25 + LM + Large Model
best_config = create_three_stage_config('accurate')
retriever_best = ThreeStageRetrieval(
    use_bm25=True,
    use_dense_retrieval=True,  # This acts as Language Model stage
    use_reranking=True,        # Large model reranking
    stage1_top_k=100,          # aâ‚ = 100 (paper setting)
    stage2_top_k=20,           # aâ‚‚ = 20 (paper setting)
    stage3_top_k=10,           # Final top-10 results
    embedding_model='keepitreal/vietnamese-sbert',  # Stage 2
    reranker_model='keepitreal/vietnamese-reranker'  # Stage 3
)

# Paper's stable method: BM25 + LM + Ensemble  
ensemble_config = ThreeStageRetrieval(
    use_bm25=True,
    use_dense_retrieval=True,
    use_reranking=True,
    stage1_top_k=100,
    stage2_top_k=20, 
    stage3_top_k=10,
    # Ensemble of 3 models (paper approach)
    ensemble_size=3
)

# Baseline comparison: BM25 + LM only
baseline_config = ThreeStageRetrieval(
    use_bm25=True,
    use_dense_retrieval=True,
    use_reranking=False,  # Skip Stage 3
    stage1_top_k=100,
    stage2_top_k=10      # Direct to final results
)
```

### **3ï¸âƒ£ Cháº¡y Demo Script**
```bash
# Náº¿u cÃ³ demo script cho Three-Stage
python demo_three_stage.py

# Hoáº·c tÃ­ch há»£p vÃ o enhanced search engine
python EnhancedSearchEngine.py
# â†’ Chá»n mode "three_stage" trong interactive interface
```

## ğŸ“Š Performance Results (Paper Experimental Data)

### **ğŸ¯ Datasets Used:**
- **Training:** MS-MARCO (502,939 queries, 8.8M documents)
- **Testing:** Zero-shot evaluation on BEIR datasets:
  - FiQA-2018, SciFact, HotpotQA
  - Evaluation metric: NDCG@10

### **ğŸ“ˆ Paper Results (NDCG@10 Scores):**

| Method | In-domain | Out-of-domain | Speed | Delay vs |
|--------|-----------|---------------|-------|----------|
|  | MS-MARCO | Average | (sec/query) | BM25+LM |
| **BM25** | 0.2294 | 0.5434 | 0.0178 | 0.01x |
| **BM25 + LM** | 0.3714 | 0.5761 | 0.1939 | 1.00x |
| **BM25 + LM + Pairwise** | 0.3889 | 0.5105 | 0.7759 | 4.00x |
| **BM25 + LM + Ensemble** | 0.3761 | 0.5922 | 0.2910 | 1.50x |
| **BM25 + LM + Large** | 0.3845 | 0.6176 | 0.2693 | 1.39x |

### **ğŸ† Key Insights tá»« Paper:**

#### **âœ… Best Method: BM25 + LM + Large Model**
```python
- Highest out-of-domain accuracy: 0.6176 NDCG@10
- Only 1.39x slower than BM25+LM baseline
- Best trade-off between accuracy vÃ  speed
- Consistently good across all test datasets
```

#### **â­ Good Alternative: BM25 + LM + Ensemble**
```python
- Stable performance: 0.5922 NDCG@10 
- Exceeds BM25+LM on ALL datasets
- 1.50x slower than baseline (reasonable)
- More robust than pairwise approach
```

#### **âŒ Poor Performer: Pairwise Model**
```python
- Good in-domain (0.3889) but poor out-of-domain (0.5105)
- 4.00x slower than baseline (too expensive!)
- Problems: token length limits, quadratic complexity
- Poor zero-shot generalization
```

### **ğŸ“Š Speed vs Accuracy Trade-off:**
```python
ğŸ” BM25 Only:
- Speed: âš¡âš¡âš¡ Fastest (0.0178s)
- Accuracy: ğŸ“Š Basic (0.5434)
- Use: Initial filtering, very fast searches

ğŸ§  BM25 + Language Model:
- Speed: â­â­ Fast (0.1939s) 
- Accuracy: ğŸ“ˆ Good (0.5761)
- Use: Standard production baseline

ğŸ¯ BM25 + LM + Large Model:
- Speed: â­ Moderate (0.2693s)
- Accuracy: ğŸ† Best (0.6176)
- Use: High-quality search, research applications

ğŸŒ BM25 + LM + Pairwise:
- Speed: âŒ Slow (0.7759s)
- Accuracy: ğŸ“‰ Poor out-of-domain (0.5105)
- Use: KhÃ´ng khuyáº¿n nghá»‹ (existing method comparison)
```

## ğŸ”§ Configuration Options

### **ğŸ›ï¸ Stage Configuration:**
```python
retriever = ThreeStageRetrieval(
    # Stage 1: BM25 settings
    use_bm25=True,
    stage1_top_k=100,           # Candidates sau BM25
    
    # Stage 2: Dense retrieval settings  
    embedding_model='keepitreal/vietnamese-sbert',
    use_dense_retrieval=True,
    stage2_top_k=50,            # Candidates sau Dense
    
    # Stage 3: Reranking settings
    reranker_model='keepitreal/vietnamese-reranker',
    use_reranking=True,
    stage3_top_k=20,            # Final results
    
    # Weight fusion
    bm25_weight=0.3,
    dense_weight=0.4,
    rerank_weight=0.3
)
```

### **âš–ï¸ Weight Tuning:**
```python
# Keyword-focused (tá»‘t cho exact matches)
weights = {'bm25_weight': 0.6, 'dense_weight': 0.3, 'rerank_weight': 0.1}

# Semantic-focused (tá»‘t cho conceptual queries)  
weights = {'bm25_weight': 0.2, 'dense_weight': 0.5, 'rerank_weight': 0.3}

# Precision-focused (tá»‘t cho research)
weights = {'bm25_weight': 0.2, 'dense_weight': 0.3, 'rerank_weight': 0.5}
```

## ğŸ“‹ Expected Output

### **ğŸ” Stage Details Output:**
```python
ğŸ” SEARCH RESULTS WITH STAGE BREAKDOWN:
========================================
Query: "BÃ  Triá»‡u sinh nÄƒm nao"

Pipeline Summary:
- Initial candidates: 5,968 chunks
- After Stage 1 (BM25): 100 candidates  
- After Stage 2 (Dense): 50 candidates
- Final results: 10 results

Stage 1 (BM25 Keyword):
- Method: BM25 keyword matching
- Candidates: 100
- Top scores: [8.42, 7.89, 7.23, 6.78, 6.45]
- Time: 0.05s

Stage 2 (Dense Semantic):  
- Method: Dense embedding similarity
- Candidates: 50
- Top scores: [0.89, 0.87, 0.84, 0.82, 0.79]
- Time: 0.20s

Stage 3 (LLM Reranking):
- Method: Cross-encoder reranking  
- Candidates: 10
- Top scores: [0.95, 0.91, 0.87, 0.83, 0.78]
- Time: 1.10s

Final Results:
[1] BÃ  Triá»‡u.md (Score: 0.95) â­â­â­
    Content: # BÃ  Triá»‡u BÃ  Triá»‡u (chá»¯ HÃ¡n: è¶™å©†, cÃ²n gá»i lÃ  Triá»‡u Trinh NÆ°Æ¡ng...
    
[2] Triá»‡u Thá»‹ Trinh.md (Score: 0.91) â­â­â­
    Content: ## Tiá»ƒu sá»­ Triá»‡u Thá»‹ Trinh sinh nÄƒm 225, máº¥t nÄƒm 248...
```

## ğŸ’¡ Best Practices

### **ğŸ¯ Khi NÃ o DÃ¹ng Three-Stage:**
```python
âœ… Excellent for:
- Research queries cáº§n Ä‘á»™ chÃ­nh xÃ¡c cao
- Complex semantic queries: "bá»‘i cáº£nh lá»‹ch sá»­ khá»Ÿi nghÄ©a"  
- Multi-aspect queries: "áº£nh hÆ°á»Ÿng kinh táº¿ chiáº¿n tranh"
- When precision > speed requirements

âš ï¸ Consider alternatives for:
- Simple keyword queries: "Há»“ ChÃ­ Minh"
- Real-time applications cáº§n < 0.1s response  
- Large-scale concurrent queries (resource intensive)
- When BM25-only results are sufficient
```

### **âš¡ Performance Optimization:**
```python
# 1. Adjust stage sizes based on corpus
small_corpus = {'stage1_top_k': 50, 'stage2_top_k': 25, 'stage3_top_k': 10}
large_corpus = {'stage1_top_k': 200, 'stage2_top_k': 100, 'stage3_top_k': 30}

# 2. Skip stages for speed
fast_search = {'use_reranking': False}  # Skip expensive Stage 3
keyword_only = {'use_dense_retrieval': False, 'use_reranking': False}

# 3. Batch processing cho multiple queries
# Process queries in batches Ä‘á»ƒ optimize GPU usage
```

### **ğŸ“Š Quality Evaluation:**
```python
# Test vá»›i known relevant queries
test_queries = [
    ("BÃ  Triá»‡u sinh nÄƒm nao", ["BÃ  Triá»‡u.md", "Triá»‡u Thá»‹ Trinh.md"]),
    ("Äiá»‡n BiÃªn Phá»§ chiáº¿n dá»‹ch", ["Äiá»‡n BiÃªn Phá»§.md", "Chiáº¿n dá»‹ch 1954.md"]),
    ("Há»“ ChÃ­ Minh cÃ¡ch máº¡ng", ["Há»“ ChÃ­ Minh.md", "CÃ¡ch máº¡ng thÃ¡ng 8.md"])
]

# Evaluate precision@k for each stage
for query, expected_docs in test_queries:
    results = retriever.retrieve_three_stage(query, ..., return_stage_details=True)
    precision = calculate_precision_at_k(results['final_results'], expected_docs)
    print(f"Query: {query} â†’ Precision@5: {precision:.3f}")
```

## ğŸ”¬ Advanced Features

### **ğŸ§  Paper's Training Details:**
```python
# Model Architecture (Paper specifications)
Language Model (Stage 2):
- Base: MiniLM (distilled from RoBERTa-Large)
- Parameters: 30M 
- Layers: 6, Hidden size: 384
- Training: Binary classification vá»›i cross-entropy loss

Large Model (Stage 3):  
- Base: MiniLM architecture
- Parameters: 81M
- Layers: 6, Hidden size: 768
- Same training approach as Stage 2

Ensemble (Stage 3):
- 3 identical MiniLM models (30M each)
- Same hyperparameters, different random seeds
- Average similarity scores across models

# Training Configuration (Paper settings)
Optimizer: Adam (Î²â‚=0.9, Î²â‚‚=0.999, Îµ=10â»â¶)
L2 regularization: 0.01
Learning rate: {1Ã—10â»âµ, 2Ã—10â»âµ, 5Ã—10â»âµ}
Epochs: {5, 10, 20, 30}
Batch size: 64
Dropout: 0.1
Max tokens: 512 (training), 256/512 (inference)
```

### **ğŸ“ˆ Adaptive Pipeline:**
```python
# Tá»± Ä‘á»™ng adjust stage sizes dá»±a trÃªn query complexity
def adaptive_config(query: str) -> Dict:
    query_complexity = analyze_query_complexity(query)
    
    if query_complexity == 'simple':
        return create_three_stage_config('fast')
    elif query_complexity == 'medium':
        return create_three_stage_config('balanced')  
    else:
        return create_three_stage_config('accurate')
```

### **ğŸ”„ Pipeline Comparison:**
```python
# So sÃ¡nh vá»›i há»‡ thá»‘ng hiá»‡n táº¡i
comparison = retriever.compare_with_current_system(
    query="BÃ  Triá»‡u khá»Ÿi nghÄ©a",
    query_tokens=['bÃ ', 'triá»‡u', 'khá»Ÿi', 'nghÄ©a'],
    current_system_results=current_results,
    top_k=10
)

print("ğŸ“Š System Comparison:")
print(f"Three-stage results: {comparison['three_stage_results']}")
print(f"Current system results: {comparison['current_system_results']}")
print(f"Pipeline efficiency: {comparison['three_stage_pipeline']}")
```

## ğŸ¯ Káº¿t Luáº­n tá»« Paper

### **ğŸ“Š Paper's Main Contributions:**

1. **ğŸ” Multi-stage architecture** giáº£i quyáº¿t trade-off giá»¯a accuracy vÃ  speed
2. **ğŸ§  Alternative to pairwise models** vá»›i better zero-shot performance  
3. **ğŸ¯ Practical deployment** vá»›i minimal speed reduction (1.39x vs 4.00x)

### **ğŸ† Paper's Key Findings:**

#### **âœ… What Works Well:**
```python
âœ… BM25 + Language Model + Large Model:
   - Best overall performance (0.6176 NDCG@10)
   - Only 39% speed penalty vs baseline
   - Consistent across all test datasets
   - Good zero-shot generalization

âœ… Model Ensemble approach:
   - Stable performance improvement
   - Minimal computational overhead
   - Robust alternative to complex models
```

#### **âŒ What Doesn't Work:**
```python
âŒ Pairwise Models (existing approach):
   - Poor zero-shot performance (0.5105 vs 0.5761 baseline)
   - 4x speed penalty (too expensive)
   - Token length limitations vá»›i long documents
   - Quadratic complexity (aâ‚‚Â² inferences)
```

### **ğŸš€ Practical Recommendations:**

#### **ğŸ“ˆ For Production Systems:**
```python
Recommended: BM25 + LM + Large Model
- Setting: aâ‚=100, aâ‚‚=20, final=10
- Expected: 7% accuracy improvement, 39% speed cost
- Best for: Quality-focused applications

Alternative: BM25 + LM + Ensemble  
- Setting: 3-model ensemble
- Expected: 3% accuracy improvement, 50% speed cost
- Best for: Robust, stable performance
```

#### **âš¡ For Speed-Critical Applications:**
```python
Baseline: BM25 + Language Model only
- Skip Stage 3 entirely  
- Still 58% better than BM25-only
- Reasonable accuracy-speed balance
```

#### **ğŸ”¬ For Research/High-Precision:**
```python
Full Pipeline: All 3 stages with tuned parameters
- Experiment vá»›i different aâ‚, aâ‚‚ values
- Consider domain-specific model fine-tuning
- Monitor convergence points (Figure 4 in paper)
```

### **ğŸ’¡ Implementation Insights:**

1. **ğŸ“Š Parameter Tuning:** Paper shows accuracy converges at aâ‚‚=30 (ensemble) or aâ‚‚=50 (large model)
2. **ğŸ¯ Trade-off Flexibility:** Easy to adjust ai parameters based on accuracy/speed requirements  
3. **ğŸ§  Zero-shot Performance:** Critical for practical deployment where domain-specific training data is expensive
4. **âš–ï¸ Ensemble Benefits:** Simple approach (same model, different seeds) provides stable improvements

### **ğŸ”® Future Developments (Paper Mentions):**
- Replace BM25 vá»›i embedding methods (DPR)
- Explore other module combinations
- Domain adaptation techniques
- More sophisticated ensemble strategies

**ğŸ“‹ Citation:**
```
@article{sasazawa2023text,
  title={Text Retrieval with Multi-Stage Re-Ranking Models},
  author={Sasazawa, Yuichi and Yokote, Kenichi and Imaichi, Osamu and Sogawa, Yasuhiro},
  journal={arXiv preprint arXiv:2311.07994},
  year={2023}
}
```