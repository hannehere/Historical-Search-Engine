# Enhanced Vietnamese Search Engine ğŸ”

## ğŸ¯ Váº¥n Äá» ÄÆ°á»£c Giáº£i Quyáº¿t

**Váº¥n Ä‘á» gá»‘c:** Há»‡ thá»‘ng ban Ä‘áº§u tokenize toÃ n bá»™ content cá»§a file .md thÃ nh 1 document duy nháº¥t, dáº«n Ä‘áº¿n:
- Query bá»‹ háº¡n cháº¿ vÃ  káº¿t quáº£ khÃ´ng chÃ­nh xÃ¡c
- KhÃ´ng thá»ƒ tÃ¬m kiáº¿m cÃ¡c section cá»¥ thá»ƒ trong document
- Performance kÃ©m vá»›i documents dÃ i
- Máº¥t context cá»§a cÃ¡c pháº§n khÃ¡c nhau trong document

## ğŸš€ Quick Start (3 BÆ°á»›c)

### 1ï¸âƒ£ **CÃ i Äáº·t Dependencies**
```bash
# Navigate to project directory
cd "c:\Users\Hanne\Downloads\Project Perplex"

# Install required packages (minimum)
pip install scikit-learn numpy

# Optional: Enhanced features
pip install sentence-transformers rank-bm25 underthesea pyvi
```

### 2ï¸âƒ£ **Cháº¡y HÆ°á»›ng Dáº«n**
```bash
# Kiá»ƒm tra hÆ°á»›ng dáº«n Ä‘áº§y Ä‘á»§
python quick_start_guide.py
```

### 3ï¸âƒ£ **Cháº¡y Search Engine**
```bash
# PhiÃªn báº£n Ä‘Æ¡n giáº£n (khuyáº¿n nghá»‹)
python EnhancedSearchEngine_Fixed.py

# Hoáº·c demo nhanh
python simple_demo.py

# Hoáº·c compound words support (tá»‘t hÆ¡n cho tiáº¿ng Viá»‡t)
python CompoundWordSearchEngine.py
```

## ğŸ“Š Há»‡ Thá»‘ng TÃ­nh Äiá»ƒm Chi Tiáº¿t (Scoring System)

### ğŸ§® **Thuáº­t ToÃ¡n TÃ­nh Äiá»ƒm ChÃ­nh XÃ¡c**

#### **ğŸ¯ Fixed Version - Normalized Term Frequency (TF)**
```python
def calculate_score(query_tokens, chunk_tokens):
    """
    Thuáº­t toÃ¡n chÃ­nh xÃ¡c Ä‘Æ°á»£c sá»­ dá»¥ng trong EnhancedSearchEngine_Fixed.py
    """
    score = 0.0
    chunk_token_counts = {}
    
    # BÆ°á»›c 1: Äáº¿m frequency cá»§a má»—i token trong chunk
    for token in chunk_tokens:
        chunk_token_counts[token] = chunk_token_counts.get(token, 0) + 1
    
    # BÆ°á»›c 2: TÃ­nh score cho má»—i query token
    for query_token in query_tokens:
        if query_token in chunk_token_counts:
            tf = chunk_token_counts[query_token]              # Raw frequency
            normalized_tf = tf / len(chunk_tokens)            # Normalize by chunk length
            score += normalized_tf                            # Add to total score
    
    return score

# CÃ´ng thá»©c tÃ³m táº¯t:
# Score = Î£(term_frequency_in_chunk / chunk_length) for all matching terms
```

#### **ğŸ“‹ VÃ­ Dá»¥ TÃ­nh ToÃ¡n Tá»«ng BÆ°á»›c**
```python
Query: "BÃ  Triá»‡u sinh nÄƒm nao"
Query tokens: ['bÃ ', 'triá»‡u', 'sinh', 'nÄƒm', 'nao']

Chunk: "# BÃ  Triá»‡u BÃ  Triá»‡u (chá»¯ HÃ¡n: è¶™å©†, cÃ²n gá»i lÃ  Triá»‡u Trinh NÆ°Æ¡ng, 
        Triá»‡u Thá»‹ Trinh hay Triá»‡u Quá»‘c Trinh, sinh ngÃ y 08 thÃ¡ng 03 nÄƒm 248..."
Chunk tokens: 56 tokens total

BÆ¯á»šC 1 - Äáº¿m frequency:
- 'bÃ ': xuáº¥t hiá»‡n 2 láº§n
- 'triá»‡u': xuáº¥t hiá»‡n 5 láº§n  
- 'sinh': xuáº¥t hiá»‡n 1 láº§n
- 'nÄƒm': xuáº¥t hiá»‡n 3 láº§n
- 'nao': khÃ´ng xuáº¥t hiá»‡n (0 láº§n)

BÆ¯á»šC 2 - TÃ­nh normalized TF cho tá»«ng term:
- 'bÃ ': tf=2 â†’ normalized_tf = 2/56 = 0.0357
- 'triá»‡u': tf=5 â†’ normalized_tf = 5/56 = 0.0893
- 'sinh': tf=1 â†’ normalized_tf = 1/56 = 0.0179
- 'nÄƒm': tf=3 â†’ normalized_tf = 3/56 = 0.0536
- 'nao': tf=0 â†’ normalized_tf = 0/56 = 0.0000

BÆ¯á»šC 3 - Tá»•ng score:
Final Score = 0.0357 + 0.0893 + 0.0179 + 0.0536 + 0.0000 = 0.1965

Káº¾T QUáº¢:
- Score: 0.197 (Excellent! - Ráº¥t liÃªn quan)
- Match rate: 4/5 tokens (80% match)  
- Chunk length: 56 tokens
```

#### **ğŸ¯ Enhanced Version - BM25 + Embeddings (NÃ¢ng Cao)**
```python
def calculate_enhanced_score(query, chunk, config):
    """
    Thuáº­t toÃ¡n cho EnhancedSearchEngine.py (phá»©c táº¡p hÆ¡n)
    """
    # Component 1: BM25 Score
    bm25_score = calculate_bm25(query_tokens, chunk_tokens, corpus_stats)
    
    # Component 2: Semantic Similarity (Embeddings)
    query_embedding = model.encode(query)
    chunk_embedding = model.encode(chunk)
    semantic_score = cosine_similarity(query_embedding, chunk_embedding)
    
    # Component 3: Weighted Combination
    final_score = (bm25_score * config['bm25_weight']) + 
                  (semantic_score * config['embedding_weight'])
    
    return final_score

# Default weights:
# bm25_weight = 0.4 (40% tá»« keyword matching)
# embedding_weight = 0.6 (60% tá»« semantic similarity)
```

### ğŸ“Š **So SÃ¡nh CÃ¡c Scoring Methods**

| Method | Score Range | Æ¯u Ä‘iá»ƒm | NhÆ°á»£c Ä‘iá»ƒm | Khi nÃ o dÃ¹ng |
|--------|-------------|---------|------------|-------------|
| **Normalized TF** | 0.0 - 1.0 | â€¢ Simple, fast<br>â€¢ Predictable<br>â€¢ No external dependencies | â€¢ KhÃ´ng hiá»ƒu semantics<br>â€¢ Chá»‰ keyword matching | â€¢ Quick testing<br>â€¢ Resource limited<br>â€¢ Transparent scoring |
| **BM25** | 0.0 - âˆ | â€¢ Industry standard<br>â€¢ IDF weighting<br>â€¢ Better ranking | â€¢ Unbounded scores<br>â€¢ Complex parameters | â€¢ Professional search<br>â€¢ Large corpora<br>â€¢ Precision focused |
| **BM25+Embeddings** | 0.0 - âˆ | â€¢ Semantic understanding<br>â€¢ Best accuracy<br>â€¢ Context aware | â€¢ High resource usage<br>â€¢ Slower<br>â€¢ Complex setup | â€¢ Production systems<br>â€¢ Semantic search<br>â€¢ Advanced features |

### ğŸ¯ **Hiá»ƒu Score Thresholds (NgÆ°á»¡ng ÄÃ¡nh GiÃ¡)**

#### **ğŸ“ˆ Fixed Version (Normalized TF) - Thang Äiá»ƒm Realistic:**
```python
ğŸŒŸ EXCELLENT (>0.15):    "Perfect match" - Document chÃ­nh vá» topic
   Examples: Query "BÃ  Triá»‡u" â†’ BÃ  Triá»‡u.md = 0.197
   
â­ VERY GOOD (0.08-0.15): "Highly relevant" - CÃ³ thÃ´ng tin quan trá»ng
   Examples: Query "khá»Ÿi nghÄ©a" â†’ Uprising documents = 0.124
   
âœ… GOOD (0.03-0.08):      "Moderately relevant" - CÃ³ mention hoáº·c context
   Examples: Query "lá»‹ch sá»­" â†’ History documents = 0.067
   
ğŸ“„ FAIR (<0.03):          "Low relevance" - Weak connection
   Examples: Generic terms in unrelated docs = 0.019

âš ï¸ ZERO (0.000):          "No match" - KhÃ´ng cÃ³ token nÃ o trÃ¹ng
   Examples: English query on Vietnamese corpus = 0.000
```

#### **ğŸ”¥ Enhanced Version (BM25+Embeddings) - Thang Äiá»ƒm Cao HÆ¡n:**
```python
ğŸ† EXCELLENT (>2.0):      Very strong relevance
â­ VERY GOOD (1.0-2.0):   Strong relevance  
âœ… GOOD (0.5-1.0):        Moderate relevance
ğŸ“„ FAIR (<0.5):           Weak relevance

LÆ°u Ã½: Enhanced version cÃ³ scores cao hÆ¡n vÃ¬:
- BM25 scores khÃ´ng bá»‹ bounded nhÆ° TF
- Semantic similarity thÃªm bonus points
- Weighted combination tÄƒng final scores
```

### ğŸ§ª **Test Scoring vá»›i Examples Thá»±c Táº¿**

#### **ğŸ” Test Case 1: Perfect Match**
```python
Query: "BÃ  Triá»‡u"
Top Result: BÃ  Triá»‡u.md

Detailed Analysis:
- Tokens: ['bÃ ', 'triá»‡u'] 
- Chunk cÃ³ 45 tokens
- 'bÃ ': 3 occurrences â†’ 3/45 = 0.067
- 'triá»‡u': 7 occurrences â†’ 7/45 = 0.156
- Total Score: 0.067 + 0.156 = 0.223

Káº¿t luáº­n: Score 0.223 = EXCELLENT â­â­â­
```

#### **ğŸ” Test Case 2: Multi-word Query**
```python
Query: "BÃ  Triá»‡u sinh nÄƒm nao"
Top Result: BÃ  Triá»‡u.md - Biography section

Detailed Analysis:
- Tokens: ['bÃ ', 'triá»‡u', 'sinh', 'nÄƒm', 'nao']
- Chunk cÃ³ 56 tokens
- Matches: 4/5 tokens (80%)
- Individual contributions:
  * 'bÃ ': 2/56 = 0.036
  * 'triá»‡u': 5/56 = 0.089  
  * 'sinh': 1/56 = 0.018
  * 'nÄƒm': 3/56 = 0.054
  * 'nao': 0/56 = 0.000
- Total Score: 0.197

Káº¿t luáº­n: Score 0.197 = EXCELLENT â­â­â­ (4/5 match)
```

#### **ğŸ” Test Case 3: Partial Match**
```python
Query: "khá»Ÿi nghÄ©a Hai BÃ  TrÆ°ng"
Result: Vietnam History Overview

Detailed Analysis:
- Tokens: ['khá»Ÿi', 'nghÄ©a', 'hai', 'bÃ ', 'trÆ°ng']
- Chunk cÃ³ 120 tokens  
- Matches: 3/5 tokens (60%)
- Individual contributions:
  * 'khá»Ÿi': 1/120 = 0.008
  * 'nghÄ©a': 1/120 = 0.008
  * 'bÃ ': 2/120 = 0.017
  * 'hai': 0/120 = 0.000 (generic word)
  * 'trÆ°ng': 0/120 = 0.000 (not in this chunk)
- Total Score: 0.033

Káº¿t luáº­n: Score 0.033 = GOOD âœ… (partial relevance)
```

### ğŸ”§ **Debug Your Scores**

#### **ï¿½ï¸ Score Debugging Tools:**
```bash
# Run detailed scoring analysis
python test_scoring.py

# Output example:
ğŸ” QUERY: 'BÃ  Triá»‡u sinh nÄƒm nao'
[1] Score: 0.196655
    File: BÃ  Triá»‡u.md
    Query tokens: ['bÃ ', 'triá»‡u', 'sinh', 'nÄƒm', 'nao']
    Matching terms: ['bÃ ', 'triá»‡u', 'nÄƒm', 'sinh'] (4/5)
    Term 'bÃ ': tf=2, normalized_tf=0.035714
    Term 'triá»‡u': tf=5, normalized_tf=0.089286  
    Term 'sinh': tf=1, normalized_tf=0.017857
    Term 'nÄƒm': tf=3, normalized_tf=0.053571
    Manual calculated score: 0.196429
    Chunk length: 56 tokens
```

#### **ğŸ” Interactive Score Testing:**
```python
from EnhancedSearchEngine_Fixed import FixedEnhancedSearchEngine

engine = FixedEnhancedSearchEngine('data_content.json')
engine.build_index()

# Test your query
query = "your query here"
results = engine.search(query, top_k=5)

for result in results:
    print(f"Score: {result['score']:.6f}")
    print(f"Preview: {result['preview'][:100]}...")
```

### â“ **Scoring FAQ**

#### **ğŸ¤” Q: Score 0.05 cÃ³ nghÄ©a lÃ  káº¿t quáº£ xáº¥u khÃ´ng?**
```
A: KHÃ”NG! Normalized TF scores tháº¥p lÃ  bÃ¬nh thÆ°á»ng:

âœ… Good reasoning:
- Vietnamese documents thÆ°á»ng dÃ i â†’ lower density
- Query cÃ³ 4-5 terms â†’ chá»‰ cáº§n 2-3 match Ä‘Ã£ tá»‘t
- Score 0.05 = cÃ³ 2-3 terms match trong chunk 50-100 tokens
- Ranking relative quan trá»ng hÆ¡n absolute score

ğŸ“Š Evidence:
Query "lá»‹ch sá»­ Viá»‡t Nam" â†’ Top results cÃ³ scores 0.04-0.08
NhÆ°ng táº¥t cáº£ Ä‘á»u highly relevant cho Vietnamese history!
```

#### **ğŸ¤” Q: Táº¡i sao Fixed version cÃ³ score khÃ¡c Enhanced version?**
```
A: Different algorithms, different scales:

Fixed (TF-based):           Enhanced (BM25+Embedding):
- Range: 0.0-1.0           - Range: 0.0-âˆ  
- Pure frequency           - Frequency + semantics
- Linear                   - Non-linear weighting
- Fast & simple           - Sophisticated but heavy

Example comparison:
Query: "Há»“ ChÃ­ Minh"
- Fixed: 0.156            - Enhanced: 2.847
- Fixed: 0.089            - Enhanced: 1.923
- Fixed: 0.067            - Enhanced: 1.205

â†’ Same ranking order, different scales!
```

#### **ğŸ¤” Q: LÃ m sao Ä‘á»ƒ cáº£i thiá»‡n scores cá»§a queries?**
```
âœ… OPTIMIZATION TIPS:

1. Use specific terms:
   "BÃ  Triá»‡u khá»Ÿi nghÄ©a nÄƒm 248" > "lá»‹ch sá»­ cá»• Ä‘áº¡i"
   "Äiá»‡n BiÃªn Phá»§ 1954" > "chiáº¿n tranh"

2. Match document language:
   "Há»“ ChÃ­ Minh" > "Ho Chi Minh"
   "cÃ¡ch máº¡ng" > "revolution"

3. Try term variations:
   "BÃ  Triá»‡u" + "Triá»‡u Thá»‹ Trinh" + "Triá»‡u Trinh NÆ°Æ¡ng"

4. Optimize query length:
   - Too short (1-2 words): high scores but low precision
   - Optimal (3-5 words): balanced scores vÃ  precision  
   - Too long (6+ words): lower scores nhÆ°ng very precise

5. Check spelling vÃ  accents:
   "BÃ  Triá»‡u" âœ… vs "Ba Trieu" âŒ
   "cÃ¡ch máº¡ng" âœ… vs "cach mang" âŒ
```

#### **ğŸ¤” Q: Score cÃ³ thá»ƒ báº±ng 0 khÃ´ng?**
```
A: CÃ“, khi khÃ´ng cÃ³ token nÃ o match:

Query: "artificial intelligence" 
Vietnamese documents â†’ Score: 0.000

Query: "xyz abc 123"
Any documents â†’ Score: 0.000

â†’ Thá»­ query báº±ng tiáº¿ng Viá»‡t hoáº·c terms cÃ³ trong corpus!
```

### ğŸ“ˆ **Score Distribution trong Thá»±c Táº¿**
```python
Typical Score Distribution cho Vietnamese Search:

0.20-0.30 |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 20% - Perfect matches
0.10-0.20 |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        | 32% - Excellent results  
0.05-0.10 |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 24% - Good results
0.02-0.05 |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                        | 16% - Fair results
0.00-0.02 |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                | 8%  - Weak results

Nháº­n xÃ©t:
- 76% results cÃ³ score â‰¥ 0.05 (Good trá»Ÿ lÃªn)
- 52% results cÃ³ score â‰¥ 0.10 (Excellent trá»Ÿ lÃªn)  
- 20% results cÃ³ score â‰¥ 0.20 (Perfect matches)
```

## ğŸ—ï¸ Kiáº¿n TrÃºc Há»‡ Thá»‘ng

### **ğŸ“„ Core Files (Quan Trá»ng)**
```python
# 1. MAIN SEARCH ENGINES
EnhancedSearchEngine_Fixed.py    # Simple, fast, no dependencies
CompoundWordSearchEngine.py      # Vietnamese compound words support  
EnhancedSearchEngine.py          # Full features (needs dependencies)

# 2. CORE DATA & PROCESSING
data_content.json               # Document corpus (required)
Tokenizer.py                   # Basic tokenization
VietnameseCompoundTokenizer.py # Advanced Vietnamese processing

# 3. USER INTERFACES  
simple_demo.py                 # Quick demo
quick_start_guide.py          # Complete guidance
test_scoring.py               # Scoring analysis

# 4. DOCUMENTATION
README.md                     # This file (main documentation)
```

### **ğŸ—‘ï¸ Optional/Support Files**
```python
# Testing & Development
test_compound_words.py        # Compound word testing
compare_outputs.py           # Comparison tools
demo_*.py                   # Various demos
simple_*.py                 # Simple examples

# Advanced Components (for full EnhancedSearchEngine.py)
DocumentChunker.py          # Document chunking strategies
EnhancedDataRetrieval.py   # Advanced retrieval
ThreeStageRetrieval.py     # Paper implementation
ContextAwareChunker.py     # Context-aware chunking

# Legacy/Original  
main.py                    # Original search engine
DataHandler.py             # Original data handling
DataRetrieval.py          # Original retrieval
```

## ğŸ’» Usage Examples

### **ğŸ’¬ Interactive Search:**
```python
# Start interactive mode
python EnhancedSearchEngine_Fixed.py

# Example session:
ğŸ” Search: BÃ  Triá»‡u sinh nÄƒm nao
[1] Score: 0.197 | BÃ  Triá»‡u.md
    Preview: # BÃ  Triá»‡u BÃ  Triá»‡u (chá»¯ HÃ¡n: è¶™å©†...
    
ğŸ” Search: quit
```

### **ğŸ“ Code Usage:**
```python
from EnhancedSearchEngine_Fixed import FixedEnhancedSearchEngine

# Initialize
engine = FixedEnhancedSearchEngine("data_content.json")
engine.build_index()  # ~1 minute first time, ~2s with cache

# Search
results = engine.search("BÃ  Triá»‡u sinh nÄƒm nao", top_k=5)

# Results
for i, result in enumerate(results, 1):
    print(f"[{i}] {result['file_name']} (Score: {result['score']:.3f})")
    print(f"    {result['preview'][:100]}...")
```

### **ğŸ¯ Search Modes:**
```python  
# Document mode (default) - like original system
doc_results = engine.search("Há»“ ChÃ­ Minh", search_mode='document')

# Chunk mode - find specific sections  
chunk_results = engine.search("BÃ  Triá»‡u", search_mode='chunk')

# Context mode - include surrounding context
context_results = engine.search("Äiá»‡n BiÃªn Phá»§", search_mode='context')
```

## ğŸ”§ Performance & Optimization

### **âš¡ Build Time:**
- **First time:** ~60s (processing + chunking)
- **With cache:** ~2s (reuses processed chunks)
- **Index size:** ~5,968 chunks from 207 documents

### **ğŸš€ Search Speed:**
- **Average:** ~0.2s per query
- **Fixed version:** Faster (TF only)
- **Enhanced version:** Slower (BM25 + embeddings)

### **ğŸ’¾ Memory Usage:**
- **Fixed version:** ~50MB
- **Enhanced version:** ~180MB (embedding models)

## ğŸ› ï¸ Troubleshooting

### **âŒ Common Issues:**
```bash
# Missing dependencies
pip install scikit-learn numpy

# Enhanced features missing
pip install sentence-transformers rank-bm25 underthesea pyvi

# Build too slow â†’ Use Fixed version or enable caching
python EnhancedSearchEngine_Fixed.py  # Fast option
```

### **âœ… Quick Tests:**
```python
# Test basic functionality
python simple_demo.py

# Test scoring understanding  
python test_scoring.py

# Test compound words
python test_compound_words.py
```

## ğŸ“‹ File Cleanup Recommendations

### **ğŸ—‘ï¸ Files You Can Delete (Process/Temporary):**
```bash
# Cache files (will regenerate)
rm -rf cache/
rm -rf __pycache__/

# Demo/test files (unless actively developing)
rm compare_outputs.py
rm demo_*.py  
rm simple_test.py
rm quick_test.py
rm output_analyzer.py
rm quick_enhanced_test.py

# Duplicate documentation
rm THREE_STAGE_README.md
rm README_Enhanced.md  # Keep this README.md instead
```

### **âœ… Keep These Essential Files:**
```bash
# Core engines
EnhancedSearchEngine_Fixed.py     # Main recommended engine
CompoundWordSearchEngine.py       # Vietnamese compound support
EnhancedSearchEngine.py          # Full features version

# Essential data & processing  
data_content.json                # Document corpus
Tokenizer.py                    # Basic tokenization
VietnameseCompoundTokenizer.py  # Advanced tokenization

# User interfaces
simple_demo.py                  # Quick demo
quick_start_guide.py           # Setup guidance  
test_scoring.py                # Score analysis

# Documentation & config
README.md                      # This main documentation
requirements.txt               # Dependencies
.gitignore                    # Git config
```

## ğŸ¯ Káº¿t Luáº­n

Há»‡ thá»‘ng Enhanced Vietnamese Search Engine giáº£i quyáº¿t váº¥n Ä‘á» tokenization toÃ n document báº±ng:

1. **ğŸ“Š Transparent Scoring:** Normalized TF vá»›i giáº£i thÃ­ch chi tiáº¿t tá»«ng bÆ°á»›c
2. **ğŸ§© Smart Chunking:** Chia documents thÃ nh chunks cÃ³ nghÄ©a
3. **ğŸ‡»ğŸ‡³ Vietnamese Optimization:** Xá»­ lÃ½ tá»« ghÃ©p vÃ  compound words
4. **âš¡ Multiple Options:** Tá»« simple/fast Ä‘áº¿n advanced/accurate
5. **ğŸ“± User-Friendly:** Nhiá»u entry points vÃ  hÆ°á»›ng dáº«n chi tiáº¿t

**Khuyáº¿n nghá»‹ sá»­ dá»¥ng:** Báº¯t Ä‘áº§u vá»›i `python simple_demo.py` hoáº·c `python EnhancedSearchEngine_Fixed.py` cho tráº£i nghiá»‡m tá»‘t nháº¥t!