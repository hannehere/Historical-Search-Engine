#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Enhanced Vietnamese Tokenizer vá»›i há»— trá»£ compound words vÃ  n-grams
"""

import re
from typing import List, Dict, Set, Tuple
from collections import defaultdict

class VietnameseCompoundTokenizer:
    """
    Tokenizer Ä‘áº·c biá»‡t cho tiáº¿ng Viá»‡t vá»›i há»— trá»£:
    - Compound words (tá»« ghÃ©p)
    - Named entities (tÃªn riÃªng) 
    - N-gram extraction
    - Context-aware processing
    """
    
    def __init__(self):
        self.compound_words = self._load_compound_words()
        self.named_entities = self._load_named_entities()
        self.stopwords = self._load_vietnamese_stopwords()
        
    def _load_compound_words(self) -> Set[str]:
        """Load danh sÃ¡ch tá»« ghÃ©p tiáº¿ng Viá»‡t quan trá»ng"""
        compound_words = {
            # Äá»‹a danh
            'viá»‡t nam', 'Ä‘Ã´ng nam Ã¡', 'báº¯c viá»‡t', 'nam viá»‡t', 
            'trung quá»‘c', 'hoa ká»³', 'liÃªn xÃ´', 'nháº­t báº£n',
            'Ä‘iá»‡n biÃªn phá»§', 'cao báº±ng', 'láº¡ng sÆ¡n', 'quáº£ng ninh',
            'hÃ  ná»™i', 'tp há»“ chÃ­ minh', 'sÃ i gÃ²n', 'Ä‘Ã  náºµng',
            
            # TÃªn ngÆ°á»i (patterns)
            'há»“ chÃ­ minh', 'nguyá»…n Ã¡i quá»‘c', 'bÃ¡c há»“', 
            'bÃ  triá»‡u', 'triá»‡u thá»‹ trinh', 'hai bÃ  trÆ°ng',
            'tráº§n hÆ°ng Ä‘áº¡o', 'lÃª lá»£i', 'nguyá»…n huá»‡',
            'vÃµ nguyÃªn giÃ¡p', 'pháº¡m vÄƒn Ä‘á»“ng',
            
            # Sá»± kiá»‡n lá»‹ch sá»­
            'chiáº¿n tranh viá»‡t nam', 'khÃ¡ng chiáº¿n chá»‘ng phÃ¡p',
            'khÃ¡ng chiáº¿n chá»‘ng má»¹', 'giáº£i phÃ³ng miá»n nam',
            'thá»‘ng nháº¥t Ä‘áº¥t nÆ°á»›c', 'cÃ¡ch máº¡ng thÃ¡ng tÃ¡m',
            'khá»Ÿi nghÄ©a', 'cÃ¡ch máº¡ng', 'giáº£i phÃ³ng',
            
            # Tá»• chá»©c
            'máº·t tráº­n giáº£i phÃ³ng', 'viá»‡t minh', 'Ä‘áº£ng cá»™ng sáº£n',
            'chÃ­nh phá»§', 'quá»‘c há»™i', 'á»§y ban', 'ban cháº¥p hÃ nh',
            
            # KhÃ¡i niá»‡m
            'Ä‘á»™c láº­p', 'tá»± do', 'hÃ²a bÃ¬nh', 'thá»‘ng nháº¥t',
            'dÃ¢n tá»™c', 'tá»• quá»‘c', 'quÃª hÆ°Æ¡ng', 'Ä‘áº¥t nÆ°á»›c',
            'lÃ£nh tá»¥', 'anh hÃ¹ng', 'liá»‡t sÄ©', 'nhÃ¢n dÃ¢n',
            
            # Thá»i gian
            'tháº¿ ká»·', 'thiÃªn niÃªn ká»·', 'triá»u Ä‘áº¡i', 'thá»i ká»³',
            'nÄƒm nay', 'nÄƒm trÆ°á»›c', 'ngÃ y nay', 'thá»i cá»•'
        }
        return compound_words
    
    def _load_named_entities(self) -> Dict[str, List[str]]:
        """Load named entities vá»›i cÃ¡c variants"""
        entities = {
            'há»“ chÃ­ minh': ['há»“ chÃ­ minh', 'nguyá»…n Ã¡i quá»‘c', 'bÃ¡c há»“', 'chá»§ tá»‹ch há»“ chÃ­ minh'],
            'bÃ  triá»‡u': ['bÃ  triá»‡u', 'triá»‡u thá»‹ trinh', 'triá»‡u trinh nÆ°Æ¡ng', 'triá»‡u quá»‘c trinh'],
            'viá»‡t nam': ['viá»‡t nam', 'nÆ°á»›c viá»‡t', 'Ä‘áº¡i viá»‡t', 'annam', 'cochinchina'],
            'Ä‘iá»‡n biÃªn phá»§': ['Ä‘iá»‡n biÃªn phá»§', 'Ä‘iá»‡n biÃªn', 'dien bien phu'],
            'hai bÃ  trÆ°ng': ['hai bÃ  trÆ°ng', 'trÆ°ng tráº¯c', 'trÆ°ng nhá»‹', 'bÃ  trÆ°ng'],
            'chiáº¿n tranh viá»‡t nam': ['chiáº¿n tranh viá»‡t nam', 'vietnam war', 'khÃ¡ng chiáº¿n chá»‘ng má»¹']
        }
        return entities
    
    def _load_vietnamese_stopwords(self) -> Set[str]:
        """Load stopwords tiáº¿ng Viá»‡t cÆ¡ báº£n"""
        return {
            'lÃ ', 'cá»§a', 'vÃ ', 'cÃ³', 'Ä‘Æ°á»£c', 'trong', 'vá»›i', 'tá»«', 'vá»', 'cho',
            'má»™t', 'hai', 'ba', 'nÃ y', 'Ä‘Ã³', 'nhá»¯ng', 'cÃ¡c', 'táº¥t cáº£', 'má»i',
            'Ä‘á»ƒ', 'sáº½', 'Ä‘Ã£', 'Ä‘ang', 'ráº¥t', 'ráº¥t nhiá»u', 'nhiá»u', 'Ã­t'
        }
    
    def extract_compound_words(self, text: str) -> List[str]:
        """Extract compound words tá»« text"""
        text_lower = text.lower()
        found_compounds = []
        
        for compound in self.compound_words:
            if compound in text_lower:
                found_compounds.append(compound)
        
        # Sort by length (longest first) Ä‘á»ƒ avoid conflicts
        found_compounds.sort(key=len, reverse=True)
        return found_compounds
    
    def extract_named_entities(self, text: str) -> List[Tuple[str, str]]:
        """Extract named entities vÃ  normalize vá» canonical form"""
        text_lower = text.lower()
        found_entities = []
        
        for canonical, variants in self.named_entities.items():
            for variant in variants:
                if variant in text_lower:
                    found_entities.append((variant, canonical))
        
        return found_entities
    
    def generate_ngrams(self, tokens: List[str], n: int = 2) -> List[str]:
        """Generate n-grams tá»« token list"""
        if len(tokens) < n:
            return []
        
        ngrams = []
        for i in range(len(tokens) - n + 1):
            ngram = ' '.join(tokens[i:i+n])
            ngrams.append(ngram)
        
        return ngrams
    
    def tokenize_basic(self, text: str) -> List[str]:
        """Basic tokenization (word-level)"""
        # Normalize text
        text = text.lower()
        text = re.sub(r'[^\w\s]', ' ', text)  # Remove punctuation
        
        # Split by whitespace
        tokens = text.split()
        
        # Filter stopwords
        tokens = [token for token in tokens if token not in self.stopwords]
        
        return tokens
    
    def tokenize_enhanced(self, text: str) -> Dict[str, List[str]]:
        """
        Enhanced tokenization vá»›i compound words vÃ  n-grams
        
        Returns:
            Dict vá»›i keys: 'tokens', 'compounds', 'entities', 'bigrams', 'trigrams'
        """
        # Basic tokens
        basic_tokens = self.tokenize_basic(text)
        
        # Extract compound words
        compounds = self.extract_compound_words(text)
        
        # Extract named entities
        entities = self.extract_named_entities(text)
        
        # Generate n-grams
        bigrams = self.generate_ngrams(basic_tokens, 2)
        trigrams = self.generate_ngrams(basic_tokens, 3)
        
        return {
            'tokens': basic_tokens,
            'compounds': compounds,
            'entities': entities,
            'bigrams': bigrams,
            'trigrams': trigrams
        }
    
    def create_search_terms(self, query: str) -> List[str]:
        """
        Táº¡o search terms optimized cho compound words
        
        Strategy:
        1. Extract compound words first
        2. Add individual tokens (except those in compounds)
        3. Add relevant bigrams
        4. Normalize entities
        """
        enhanced = self.tokenize_enhanced(query)
        
        search_terms = []
        used_tokens = set()
        
        # 1. Add compound words (highest priority)
        for compound in enhanced['compounds']:
            search_terms.append(compound)
            # Mark component tokens as used
            compound_tokens = compound.split()
            used_tokens.update(compound_tokens)
        
        # 2. Add normalized entities
        for variant, canonical in enhanced['entities']:
            if canonical not in search_terms:
                search_terms.append(canonical)
            # Mark variant tokens as used
            variant_tokens = variant.split()
            used_tokens.update(variant_tokens)
        
        # 3. Add remaining individual tokens
        for token in enhanced['tokens']:
            if token not in used_tokens and len(token) > 2:
                search_terms.append(token)
        
        # 4. Add meaningful bigrams (if no compounds found)
        if len(enhanced['compounds']) == 0 and len(enhanced['bigrams']) > 0:
            # Add top 2 bigrams
            for bigram in enhanced['bigrams'][:2]:
                if bigram not in search_terms:
                    search_terms.append(bigram)
        
        return search_terms


def test_compound_tokenizer():
    """Test compound tokenizer"""
    
    print("ðŸ”¬ TESTING COMPOUND TOKENIZER")
    print("=" * 60)
    
    tokenizer = VietnameseCompoundTokenizer()
    
    test_queries = [
        "Viá»‡t Nam",
        "Há»“ ChÃ­ Minh", 
        "BÃ  Triá»‡u sinh nÄƒm nao",
        "khá»Ÿi nghÄ©a Hai BÃ  TrÆ°ng",
        "chiáº¿n dá»‹ch Äiá»‡n BiÃªn Phá»§",
        "cÃ¡ch máº¡ng thÃ¡ng tÃ¡m",
        "khÃ¡ng chiáº¿n chá»‘ng PhÃ¡p",
        "giáº£i phÃ³ng miá»n Nam"
    ]
    
    for query in test_queries:
        print(f"\nðŸ“ Query: '{query}'")
        
        # Enhanced tokenization
        enhanced = tokenizer.tokenize_enhanced(query)
        
        print(f"   Basic tokens: {enhanced['tokens']}")
        print(f"   Compounds: {enhanced['compounds']}")
        print(f"   Entities: {enhanced['entities']}")
        print(f"   Bigrams: {enhanced['bigrams']}")
        
        # Search terms
        search_terms = tokenizer.create_search_terms(query)
        print(f"   ðŸŽ¯ Search terms: {search_terms}")
        
        print("-" * 40)


def compare_tokenization_approaches():
    """So sÃ¡nh cÃ¡c cÃ¡ch tokenize"""
    
    print("\nâš–ï¸ COMPARING TOKENIZATION APPROACHES")
    print("=" * 60)
    
    # Simple tokenizer inline
    class SimpleTokenizer:
        def tokenize(self, text):
            text = text.lower()
            text = re.sub(r'[^\w\s]', ' ', text)
            return text.split()
    
    simple_tokenizer = SimpleTokenizer()
    compound_tokenizer = VietnameseCompoundTokenizer()
    
    test_cases = [
        "Viá»‡t Nam lÃ  quÃª hÆ°Æ¡ng",
        "Há»“ ChÃ­ Minh lÃ£nh tá»¥", 
        "khá»Ÿi nghÄ©a BÃ  Triá»‡u",
        "chiáº¿n dá»‹ch Äiá»‡n BiÃªn Phá»§"
    ]
    
    for text in test_cases:
        print(f"\nðŸ“„ Text: '{text}'")
        
        # Simple approach
        simple_tokens = simple_tokenizer.tokenize(text)
        print(f"   Simple: {simple_tokens}")
        
        # Compound approach
        search_terms = compound_tokenizer.create_search_terms(text)
        print(f"   Compound: {search_terms}")
        
        # Analysis
        simple_score = len([t for t in simple_tokens if len(t) > 2])
        compound_score = len(search_terms)
        
        print(f"   â†’ Simple: {simple_score} meaningful terms")
        print(f"   â†’ Compound: {compound_score} search terms")
        print(f"   â†’ Improvement: {compound_score - simple_score:+d}")


if __name__ == "__main__":
    test_compound_tokenizer()
    compare_tokenization_approaches()