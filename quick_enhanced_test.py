"""
quick_enhanced_test.py
Quick test for Enhanced Search Engine without heavy dependencies
"""

def test_enhanced_engine():
    """Quick test c·ªßa Enhanced Search Engine"""
    
    print("üöÄ QUICK ENHANCED SEARCH ENGINE TEST")
    print("=" * 60)
    
    try:
        # Test import
        print("üìã Testing imports...")
        
        # Test basic imports first
        import json
        import time
        print("  ‚úÖ Basic imports OK")
        
        # Test if data file exists
        import os
        data_path = "data_content.json"
        if os.path.exists(data_path):
            print(f"  ‚úÖ Data file exists: {data_path}")
            
            with open(data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"  ‚úÖ Loaded {len(data)} documents")
        else:
            print(f"  ‚ùå Data file not found: {data_path}")
            return
        
        # Test enhanced tokenizer
        print("\nüìù Testing Enhanced Vietnamese Tokenizer...")
        try:
            from EnhancedVietnameseTokenizer import EnhancedVietnameseTokenizer
            tokenizer = EnhancedVietnameseTokenizer(use_stopwords=True, enable_ner=False)
            
            test_text = "B√† Tri·ªáu l√† n·ªØ t∆∞·ªõng anh h√πng c·ªßa Vi·ªát Nam"
            tokens = tokenizer.tokenize(test_text)
            print(f"  ‚úÖ Tokenizer working: '{test_text}' ‚Üí {len(tokens)} tokens")
            print(f"      Tokens: {' | '.join(tokens[:5])}...")
            
            stats = tokenizer.get_stopwords_stats()
            print(f"  ‚úÖ Stopwords: {stats['total_stopwords']} total")
            
        except Exception as e:
            print(f"  ‚ùå Enhanced tokenizer failed: {e}")
            print("  üîÑ Falling back to basic tokenizer test...")
            
            try:
                from Tokenizer import VietnameseTokenizer
                basic_tokenizer = VietnameseTokenizer()
                tokens = basic_tokenizer.tokenize("B√† Tri·ªáu anh h√πng")
                print(f"  ‚úÖ Basic tokenizer working: {len(tokens)} tokens")
            except Exception as e2:
                print(f"  ‚ùå Basic tokenizer also failed: {e2}")
        
        # Test context-aware chunker
        print("\nüß© Testing Context-Aware Chunker...")
        try:
            from ContextAwareChunker import VietnameseContextAwareChunker
            chunker = VietnameseContextAwareChunker(chunk_size=100, overlap_size=20)
            
            sample_doc = data[0]
            chunks = chunker.chunk_document(sample_doc['content'], sample_doc['filename'])
            print(f"  ‚úÖ Chunker working: {len(chunks)} chunks created")
            
            if chunks:
                sample_chunk = chunks[0]
                print(f"      Sample chunk type: {sample_chunk.chunk_type}")
                print(f"      Sample content: {sample_chunk.content[:80]}...")
                
        except Exception as e:
            print(f"  ‚ùå Context-aware chunker failed: {e}")
        
        # Test Three-Stage Retrieval
        print("\nüéØ Testing Three-Stage Retrieval...")
        try:
            from ThreeStageRetrieval import create_three_stage_config
            config = create_three_stage_config('fast')  # BM25 only for speed
            print(f"  ‚úÖ Three-stage config created: {config}")
            
        except Exception as e:
            print(f"  ‚ùå Three-stage retrieval failed: {e}")
        
        print("\n" + "=" * 60)
        print("‚úÖ QUICK TEST COMPLETED")
        print("üí° If you see mostly green checkmarks, the system is working!")
        print("‚ùå If you see errors, check the specific component dependencies")
        
    except Exception as e:
        print(f"‚ùå CRITICAL ERROR: {e}")
        import traceback
        traceback.print_exc()


def test_simple_search():
    """Test simple search functionality"""
    
    print("\nüîç SIMPLE SEARCH TEST")
    print("=" * 40)
    
    try:
        # Load data
        import json
        with open("data_content.json", 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # Simple text search
        query = "B√† Tri·ªáu"
        matches = []
        
        for doc in data:
            content = doc.get('content', '').lower()
            if query.lower() in content:
                matches.append({
                    'filename': doc.get('filename', 'Unknown'),
                    'score': content.count(query.lower()),
                    'preview': content[:200] + "..."
                })
        
        # Sort by relevance
        matches.sort(key=lambda x: x['score'], reverse=True)
        
        print(f"üîç Search for: '{query}'")
        print(f"üìã Found: {len(matches)} matches")
        
        for i, match in enumerate(matches[:3], 1):
            print(f"\n[{i}] {match['filename']} (score: {match['score']})")
            print(f"    {match['preview']}")
        
    except Exception as e:
        print(f"‚ùå Simple search failed: {e}")


if __name__ == "__main__":
    test_enhanced_engine()
    test_simple_search()